<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://ac3lab.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ac3lab.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-01-11T13:40:58+00:00</updated><id>https://ac3lab.github.io/feed.xml</id><title type="html">blank</title><subtitle>Marrying academic rigor with practical applications to benefit sports </subtitle><entry><title type="html">Running — Part 2: How a Machine Learning model allows me, through an API, to level my runs</title><link href="https://ac3lab.github.io/blog/2024/post_running_model/" rel="alternate" type="text/html" title="Running — Part 2: How a Machine Learning model allows me, through an API, to level my runs"/><published>2024-01-10T11:12:00+00:00</published><updated>2024-01-10T11:12:00+00:00</updated><id>https://ac3lab.github.io/blog/2024/post_running_model</id><content type="html" xml:base="https://ac3lab.github.io/blog/2024/post_running_model/"><![CDATA[<figure> <picture> <img src="/assets/img/Posts_Images/2024-01-01-post_running_model_Images/15.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="justify"> Recently, I did an exploratory analysis of my runs using data exported from the Nike Run Club app, and it helped me make better decisions in sport. I ran more, now having a better understanding of my training performance. All of this provided me insights that I decided to do more: I trained a model that clusters my runs and built an API that allows me to, with each new run, query this model to understand the "level" this run is, based on all my other runs. </p> <p>Introducing</p> <p align="justify"> As I already knew my data because the EDA I had done, the first step was to train the model. My goal was to find patterns in my runs, and once I found these patterns, I wanted to segment them in groups with different characteristics, so that, with each new run, I could identify which of these groups it would be included in. There are many clustering algorithms, and like any other machine learning problem, there isn't the best approach. To simplify the selection process, in addition to experimentation, we must take in consideration some factors, as the characteristics of the clusters, the features of the dataset, and possible outliers. In this case, I was familiar with my data in space and believed that one of the famous clustering algorithm could work well: [K-Means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). Without going in too much details, K-Means is an algorithm that groups data in k clusters with same variance, minimizing inertia (sum of squared distances of samples to the nearest cluster center). However, like any other algorithm, it also has its limitations, and we need understand them in the evaluation process. In short, the algorithm begins randomly generating k centroids (central points of clusters), to which each record is assigned based on the centroid with the smallest distance (forming a cluster). After that, the model calculates the average values of the points in each cluster and uses this value to reposition the centroids. This process is repeated until the positions of the centroids converge or a specified stopping criterion is met. In other words, as I wrote above, the goal of this task is to choose centroids that the inertia is minimized. It's important to note that as the initialization step is random (a non-deterministic algorithm), we should to execute multiple initializations with different centroid seeds to find the best result. </p> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-01-post_running_model_Images/16.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="justify"> As it's not the purpose of this post to address deeply about clustering algorithm, I take this opportunity to provide the entire [documentation](https://scikit-learn.org/stable/modules/clustering.html) in scikit-learn. Additionally, you can find a great post on some of these algorithms in practice at this [link](https://machinelearningmastery.com/clustering-algorithms-with-python/). </p> <p>The model</p> <p align="justify"> As I have already realized data [exploration](https://ac3lab.github.io/blog/2024/post_running/) step, I will go straight to training the model. Always remembering that in the preprocessing step, the features need to be resized to the same scale, what we call standardization. This process rescales the data to the set has a mean close to 0 and a standard deviation close to 1, optimizing the learning of distance-based algorithms. For this, I used the StandardScaler() function. Regarding the algorithm, the K-Means doesn't "decide" the ideal number of clusters. We need to do that, and I used a simple technique: Elbow Method. When we increase the number of clusters, the inertia decreases, meaning the distance from each point to its nearest centroid becomes smaller. What we need is this distance as small as possible, as we need the optimal point that minimizes the number of clusters and the variance in each cluster. </p> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-01-post_running_model_Images/17.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="justify"> The plot above is a plot of inertia (within-clusters sum-of-squares - wcss) when we increase k. The "elbow" point indicates a kind of trade-off between error and the number of clusters, meaning when the wcss no longer significantly decreases with each iteration, and there aren't no substantial gains with an increase in k. For my problem, 5 would be the optimal number of k, but just to confirm, I used a package called kneed, that finds the point on the line where the curvature is maximum. </p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cknl</span> <span class="o">=</span> <span class="nc">KneeLocator</span><span class="p">(</span><span class="n">n_cluster</span><span class="p">,</span> <span class="n">wcss</span><span class="p">,</span> <span class="n">curve</span><span class="o">=</span><span class="sh">"</span><span class="s">convex</span><span class="sh">"</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="sh">"</span><span class="s">decreasing</span><span class="sh">"</span><span class="p">)</span>
<span class="n">knl</span><span class="p">.</span><span class="n">elbow</span>
</code></pre></div></div> <p align="justify"> According to the documentation, inertia can be recognized as a measure of how internally cohesive the clusters are, but it has some disadvantages: </p> <blockquote> Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes. Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”). Running a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to k-means clustering can alleviate this problem and speed up the computations. </blockquote> <p>Knowing the optimal number of clusters, I can adjust the model to the data:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">km</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">km</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></div> <p align="justify"> With the trained model, I plotted a graph with the distance vs. pace of my races, already segmented in their respective interpreted clusters: </p> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-01-post_running_model_Images/18.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Finally, I saved:</p> <ul> <li>The trained model;</li> <li>The scaler;</li> <li>A dict with cluster labels.</li> </ul> <p>Building the API and making requests</p> <p align="justify"> To build the API I used Flask, a micro framework for web development and written in Python. It's very lightweight, simple to use and so much used for building Rest APIs. </p> <p>Creating the file.py</p> <p align="justify"> 1) Import of the necessary libraries and modules. I also imported the data_preparing.py file that I created in class format, which contains all the necessary transformations that the data needs for prediction; 2) Loading of the model and labels that were exported in the training process; 3) Assigning Flask in the variable app; 4) Creation of the endpoint that bring the prediction using the data passed through POST, which has been added to the variable 'data_json'; 5) Initializing the Flask that will receive requests on port 5000, running locally. </p> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-01-post_running_model_Images/19.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>With file.py ready, just start it in the terminal:</p> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-01-post_running_model_Images/20.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="justify"> To test it I will use Postman, a tool that allows to run API tests and make requests in general. To do this, I will take a data from some run that wasn't used in model training: </p> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-01-post_running_model_Images/21.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>On Postman:</p> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-01-post_running_model_Images/22.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="justify"> At this moment, the model's response wasn't a surprise to me, because I knew my information (my sample of runs was small). However, as my dataset grows, it becomes a bit out of control, and that's where modeling shortens the paths. Another example of how a machine learning model can help us in real life. </p> <p>References</p> <p>https://scikit-learn.org/stable/modules/clustering.html#k-means https://realpython.com/k-means-clustering-python/ https://medium.com/data-hackers/como-fazer-uma-api-rest-para-modelos-de-m-l-com-flask-e-documenta%C3%A7%C3%A3o-swagger-7e74bfa19137</p>]]></content><author><name></name></author><category term="Running,"/><category term="Python,"/><category term="&apos;Machine"/><category term="Learning&apos;"/><category term="Running;"/><category term="Python;"/><category term="&quot;Machine"/><category term="Learning&quot;"/><summary type="html"><![CDATA[Deployed! Construction of an API for querying and evaluating the level of new races based on a previously trained clustering model]]></summary></entry><entry><title type="html">Running — Part 1: Extracting my Nike Run Club data and exploring in python to evaluate my performance</title><link href="https://ac3lab.github.io/blog/2024/post_running/" rel="alternate" type="text/html" title="Running — Part 1: Extracting my Nike Run Club data and exploring in python to evaluate my performance"/><published>2024-01-06T11:12:00+00:00</published><updated>2024-01-06T11:12:00+00:00</updated><id>https://ac3lab.github.io/blog/2024/post_running</id><content type="html" xml:base="https://ac3lab.github.io/blog/2024/post_running/"><![CDATA[<figure> <picture> <img src="/assets/img/Posts_Images/2024-01-06-post_running-Images/1.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="justify"> Anyone who knows me knows that I love sport. Since I was a kid, I always was influenced to practice something and never went long without it. Some sports I've played: swimming, futsal, handball (I still play) and now running, my most recent addiction. </p> <p align="justify"> A few months ago, running has become part of my routine. Every week I try to run 2 or 3 times, and always outdoors (I hate treadmills with all my might). To measure my performance, I have been running with the Nike Run Club (NRC) app and recording my runs on it. Looking my data in the app, I felt the need to do more, some analysis of my performance. And is here that another passion comes into to play: data science. </p> <p align="justify"> Anyone who also knows me knows that I am a data scientist and rarely believe in anything or make any decision without evidence. It's a bit of cliche, but analyzing everything is also part of my routine, and so, why not bring the data to my side, and like this, who knows, it can help me make decisions that get better my results? This is what I want to start to test. Let's code! </p> <p>Data extraction</p> <p align="justify"> Firstly, I needed the data from my runs. Doing a quick research, I realized that Nike didn't officially instrument to data export, and for get it would be necessary to take alternative paths. </p> <p align="justify"> I used a program called nrc-exporter (created by some curious programmer), and it exports data to a local directory easily and quickly. As a first step, we need to install the program in the terminal using pip install nrc-exporter. The annoying part is that we need to provide the program with authentication tokens that Nike generates when you do a login, what is simple but so manual. </p> <p align="justify"> 1) Go to the Nike website and do a login normally; 2) Open Developer tools &gt; Application. Then just copy the token like this image: </p> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-06-post_running-Images/2.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="justify"> Now, back in the terminal, just run nrc-exporter with the token, like this: </p> <p>nrc-exporter -t token</p> <p>You will see this execution:</p> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-06-post_running-Images/3.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="justify"> It's ready! All data will be downloaded to a folder called activities and will be inside the folder where you run the script. The file's format will be .json. </p> <p align="justify"> If this process doesn't work, there are other ways. I'll leave the link [here](https://pypi.org/project/nrc-exporter/) if you want to know more about nrc-exporter. </p> <p>Exploratory</p> <p align="justify"> I loaded the files in a jupyter notebook to do the analysis. As all files are .json format, using the pandas json_normalize() function helps a lot. </p> <p align="justify"> After the preprocessing, creating, selecting and transforming the variables — as the average pace, for example, which comes in total minutes and not in minutes'seconds' how we normally use —, I wanted to analyze, my final dataframe looked like this: </p> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-06-post_running-Images/4.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To contextualize, some descriptive statistics:</p> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-06-post_running-Images/5.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="justify"> I analyzed a sample of races in the period of 137 days (a little over 4 months). During this period, I ran 31 races totaling 180km! I also calculated the interval average in days between one race and another. Excluding the period that I was unable to run (20 days), that I understood as an outlier that shouldn't be taken in consideration, this interval was 4 days. </p> <p align="justify"> Considering these 31 races, the average distance was 5.81km and my average pace was 6'24'' per km. Additionally, with these runs, I burned a total of 9981 calories! </p> <p>Let’s to the graphical analyses:</p> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-06-post_running-Images/6.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="justify"> The graph above illustrates the evolution of my average pace throughout the runs and the overall average. We can observe that in my recent runs, my pace has been above this average. </p> <p align="justify"> As I mentioned above, I created a variable called "days without running" to try to understand if my pace increased with many days without running. The peaks on the gray line follow the peaks on the purple line in the graph below, confirming what I already suspected: an extended period without exercise may be one of the variables affecting my time. </p> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-06-post_running-Images/7.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="justify"> Something a little obvious, but it's also cool to visualize: the opposite trajectory of the average pace and average speed. </p> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-06-post_running-Images/8.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Let’s follow:</p> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-06-post_running-Images/9.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="justify"> The graph above shows the progression of distances in my runs. The majority of my runs are in the range of 5km, and when I run more than that, I run the double. This is what pushes the average up. </p> <p>When crossing the average pace data with the distance, we have:</p> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-06-post_running-Images/10.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="justify"> The distribution of my pace in the 5km runs is varied, which doesn't surprise me. Another thing I confirmed looking the image above was the increase in my pace as I increase the distance. There's another aspect that I need to work on. </p> <p>To analyze calories, I plotted two graphs:</p> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-06-post_running-Images/11.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-06-post_running-Images//12.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="justify"> The first one makes clear the positive correlation between calories and distance, meaning if you want to burn more calories, run more (remembering that correlation doesn't imply causality, but in this case, I know it does). The second one doesn't show a correlation between calories and speed. Note that many times I burned almost the same amount of calories, running at different average speeds (probably during the many 5km runs). </p> <p align="justify"> Another feeling I had was that on cloudy days, my performance was a bit better. No sun, and a little breeze, you know how it is... But that's not what the data showed me. The boxplot below suggests that on "clear days," the distribution of my speed is the "best": </p> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-06-post_running-Images/13.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Finally, the distribution of elevation and descent on my routes:</p> <figure> <picture> <img src="/assets/img/Posts_Images/2024-01-06-post_running-Images/14.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Relevant insights</p> <p align="justify"> 1) Always run to try to keep/lower my pace for distances that I'm already used to running and consequently be able to improve for longer distances; 2) I need to run longer distances, even with a bad time, if I want to lose more calories; 3) The weather, apparently, doesn't have impact on my performance; 4) Apparently elevation gain doesn't have great correlations with running time and calories. Therefore, I can keep my usual routes if I wanted to change some of these variables. </p> <p>Some observations for who want to replicate the analysis:</p> <p align="justify"> 1) It's possible to find the latitude and longitude of the routes in the data, making it possible to georeference the races. In my case I didn't think it was relevant to use, since I almost always run in the same place. 2) As wrote above, the average pace data comes in total minutes, so if like me, you want to look at this more friendly and usual — minutes'seconds'' — it is necessary to treat this variable. 3) Also as wrote above, I didn't find the time information per section, so I preferred not to use it. </p> <p align="justify"> There are a lot of people entering/wanting to enter the data analytics/data science area and who need data to train programming or even to build their portfólios. The message I would also share with this post is that there is nothing cooler than using our data for this. In addition to using them for our benefit, generating insights that help us make better decisions (for running in my case), we address topics as treatment and transformation of variables, outliers, data quality and others, clarifying how all of this works in the real life. </p> <p align="justify"> Any suggestions are welcome! The complete analysis code can be found on my [github](https://github.com/nathaliatito). </p>]]></content><author><name></name></author><category term="Running;"/><category term="Python"/><category term="Running;"/><category term="Python"/><summary type="html"><![CDATA[How I used data science methods to evaluate my running performance]]></summary></entry></feed>